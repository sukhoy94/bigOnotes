# bigOnotes

### what is big o notation?
Big O notation is a way to describe the performance or efficiency of an algorithm in computer science. It helps us understand how an algorithm's running time (or space usage) scales with the size of the input data.

In simple terms, Big O notation tells us how fast or slow an algorithm will be as the amount of data it needs to process gets bigger. It provides an upper bound on the worst-case scenario for how the algorithm's performance will behave.
